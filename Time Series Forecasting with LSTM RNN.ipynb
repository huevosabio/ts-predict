{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Time Series Forecast in Python\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "\n",
    "Sources:\n",
    "http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import math\n",
    "import keras\n",
    "\n",
    "# jupyter command - allows plots to show up\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics       import mean_squared_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigns a Seed to Ensure reproducable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fixes the random number seed to ensure reproducible results\n",
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports a csv data set using Panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataframe = pd.read_csv('ignored_assets/AirPassengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-RNN is sensitive to large numbers, so normalizing is a good idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splits the data into training (first 2/3) and test sets (last 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 47)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "print(dataset[:,0:0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This fn takes two args and returns two datasets: \n",
    "\n",
    "A numpy array to be converted into a dataset, and the look_back (default to 1) which his the number of previous values used to predict the next value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses the Above Function to Create the Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Sets are in the Form [sampls, features] so We Reshape to be [samples, time-steps, features]\n",
    "\n",
    "<b>Samples</b>: Each incident\n",
    "<br/>\n",
    "<b>Time Steps</b>: The observations that led up to the incident\n",
    "<br/>\n",
    "<b>Features</b>: The variables (or the data) observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses the LSTM RNN from the Keras Library to Train a Neural Network with the Training Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2s - loss: 0.0103\n",
      "Epoch 2/100\n",
      "1s - loss: 0.0058\n",
      "Epoch 3/100\n",
      "0s - loss: 0.0053\n",
      "Epoch 4/100\n",
      "1s - loss: 0.0050\n",
      "Epoch 5/100\n",
      "0s - loss: 0.0047\n",
      "Epoch 6/100\n",
      "0s - loss: 0.0044\n",
      "Epoch 7/100\n",
      "1s - loss: 0.0044\n",
      "Epoch 8/100\n",
      "1s - loss: 0.0042\n",
      "Epoch 9/100\n",
      "0s - loss: 0.0041\n",
      "Epoch 10/100\n",
      "1s - loss: 0.0040\n",
      "Epoch 11/100\n",
      "0s - loss: 0.0039\n",
      "Epoch 12/100\n",
      "0s - loss: 0.0039\n",
      "Epoch 13/100\n",
      "1s - loss: 0.0038\n",
      "Epoch 14/100\n",
      "1s - loss: 0.0039\n",
      "Epoch 15/100\n",
      "1s - loss: 0.0037\n",
      "Epoch 16/100\n",
      "1s - loss: 0.0038\n",
      "Epoch 17/100\n",
      "1s - loss: 0.0037\n",
      "Epoch 18/100\n",
      "1s - loss: 0.0036\n",
      "Epoch 19/100\n",
      "0s - loss: 0.0036\n",
      "Epoch 20/100\n",
      "0s - loss: 0.0035\n",
      "Epoch 21/100\n",
      "1s - loss: 0.0036\n",
      "Epoch 22/100\n",
      "1s - loss: 0.0035\n",
      "Epoch 23/100\n",
      "1s - loss: 0.0034\n",
      "Epoch 24/100\n",
      "0s - loss: 0.0033\n",
      "Epoch 25/100\n",
      "1s - loss: 0.0033\n",
      "Epoch 26/100\n",
      "1s - loss: 0.0034\n",
      "Epoch 27/100\n",
      "1s - loss: 0.0036\n",
      "Epoch 28/100\n",
      "1s - loss: 0.0034\n",
      "Epoch 29/100\n",
      "1s - loss: 0.0033\n",
      "Epoch 30/100\n",
      "0s - loss: 0.0032\n",
      "Epoch 31/100\n",
      "0s - loss: 0.0031\n",
      "Epoch 32/100\n",
      "0s - loss: 0.0032\n",
      "Epoch 33/100\n",
      "1s - loss: 0.0032\n",
      "Epoch 34/100\n",
      "1s - loss: 0.0031\n",
      "Epoch 35/100\n",
      "0s - loss: 0.0031\n",
      "Epoch 36/100\n",
      "1s - loss: 0.0029\n",
      "Epoch 37/100\n",
      "1s - loss: 0.0030\n",
      "Epoch 38/100\n",
      "1s - loss: 0.0029\n",
      "Epoch 39/100\n",
      "1s - loss: 0.0029\n",
      "Epoch 40/100\n",
      "1s - loss: 0.0029\n",
      "Epoch 41/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 42/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 43/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 44/100\n",
      "0s - loss: 0.0028\n",
      "Epoch 45/100\n",
      "0s - loss: 0.0027\n",
      "Epoch 46/100\n",
      "1s - loss: 0.0028\n",
      "Epoch 47/100\n",
      "1s - loss: 0.0027\n",
      "Epoch 48/100\n",
      "1s - loss: 0.0026\n",
      "Epoch 49/100\n",
      "1s - loss: 0.0027\n",
      "Epoch 50/100\n",
      "1s - loss: 0.0026\n",
      "Epoch 51/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 52/100\n",
      "0s - loss: 0.0025\n",
      "Epoch 53/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 54/100\n",
      "0s - loss: 0.0026\n",
      "Epoch 55/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 56/100\n",
      "1s - loss: 0.0025\n",
      "Epoch 57/100\n",
      "1s - loss: 0.0025\n",
      "Epoch 58/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 59/100\n",
      "0s - loss: 0.0024\n",
      "Epoch 60/100\n",
      "1s - loss: 0.0024\n",
      "Epoch 61/100\n",
      "1s - loss: 0.0023\n",
      "Epoch 62/100\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the Model Trained Above, We Have the Model Attempt to Generate the Last Third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3em; text-align: center\">Making The LSTM-RNN Fit The Original Data</h1>\n",
    "\n",
    "At first, I renamed many of the variables from the above to distinguish between the separate cases. However, I decided it more efficient to generalize the LSTM-RNN into a function for simpification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again With The Seeding for Reproducable Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads the Original Data Set, and Prepares them for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interval between each dataset (original data in 5 minute intervals)\n",
    "time_grouping = '1H'\n",
    "\n",
    "# load the dataset\n",
    "car_dataframe = pd.read_csv('ignored_assets/paxout_table.csv', engine='python', nrows=3000)\n",
    "car_dataframe['time_bucket'] = pd.to_datetime(car_dataframe['time_bucket'])\n",
    "car_dataframe = car_dataframe.set_index('time_bucket')\n",
    "car_dataframe['total'] = car_dataframe.sum(axis=1)\n",
    "car_dataframe['day_hour'] = car_dataframe.index.round(time_grouping)\n",
    "\n",
    "# datasets for two districts in particular\n",
    "car_dataframe_station_1 = car_dataframe[['1afd7afbc81ecc1b13886a569d869e8a', 'day_hour']].groupby('day_hour').sum();\n",
    "car_dataset_station_1 = car_dataframe_station_1.values\n",
    "car_dataset_station_1 = car_dataset_station_1.astype('float32')\n",
    "\n",
    "car_dataframe_station_2 = car_dataframe[['d4ec2125aff74eded207d2d915ef682f', 'day_hour']].groupby('day_hour').sum();\n",
    "car_dataset_station_2 = car_dataframe_station_2.values\n",
    "car_dataset_station_2 = car_dataset_station_2.astype('float32')\n",
    "\n",
    "# The data set with the sum of all the cars out at the \n",
    "car_dataframe = car_dataframe[['total','day_hour']].groupby('day_hour').sum()\n",
    "\n",
    "car_dataset = car_dataframe.values\n",
    "car_dataset = car_dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized the Above Described Process to Work With Most Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainAndPredict(dataset, look_2back=1):\n",
    "    # normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "    \n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    print(len(train), len(test))\n",
    "\n",
    "    # reshape into X=t and Y=t+1\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # create and fit the LSTM network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    temptrainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    temptestY = scaler.inverse_transform([testY])\n",
    "    \n",
    "    # shift train predictions for plotting\n",
    "    trainPredictPlot = np.empty_like(dataset)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(dataset)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "    \n",
    "    # Shows the original data before attempting to predict\n",
    "    plt.plot(scaler.inverse_transform(dataset))\n",
    "    plt.plot(trainPredictPlot)\n",
    "    plt.plot(testPredictPlot)\n",
    "    plt.show()\n",
    "    \n",
    "    # Does the Training\n",
    "    model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "    \n",
    "    # make predictions\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY = scaler.inverse_transform([trainY])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY = scaler.inverse_transform([testY])\n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    \n",
    "    # shift train predictions for plotting\n",
    "    trainPredictPlot = np.empty_like(dataset)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(dataset)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "    # plot baseline and predictions\n",
    "    plt.plot(scaler.inverse_transform(dataset))\n",
    "    plt.plot(trainPredictPlot)\n",
    "    plt.plot(testPredictPlot)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainAndPredict(car_dataset_station_1, look_back=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainAndPredict(car_dataset_station_2, look_back=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAndPredict(car_dataset, look_back=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
