{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import math\n",
    "import keras\n",
    "from scipy.stats  import norm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras        import backend as K\n",
    "from datetime     import datetime\n",
    "from keras.optimizers      import RMSprop\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics       import mean_squared_error\n",
    "from matplotlib.pylab      import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "# number of inputs to be fed\n",
    "look_back = 288\n",
    "# number of outputs to be generated\n",
    "look_forward = 24\n",
<<<<<<< HEAD:4 - Expanding Neural Net's Scope.ipynb
    "# the scope of the LSTM Model\n",
    "stations = 66\n",
=======
    "# number of locations, aka columns in the data\n",
    "districts = 66\n",
>>>>>>> f2f9d2af909e4f876ba74a8efc9a15e52a7d0aa1:5 - Expanding Neural Net's Scope.ipynb
    "\n",
    "# convert an array of values into a dataset matrix, adjusted to make a dateset that is 66 wide\n",
    "def create_dataset(dataset, look_back=1, look_forward=2):\n",
    "    dataX, dataY = [], []\n",
    "    np.array(dataY)\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),:]\n",
    "        # Makes sure that the array isn't the last 2 or 3 or whatever bits. It has to be the full 24\n",
    "        if len(dataset[i + look_back:i+look_back+look_forward, 0]) == look_forward:\n",
    "            dataX.append(a.T)\n",
    "            dataY.append(dataset[i + look_back:i+look_back+look_forward, :].T)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def predict_with_uncertainty(model, dataset, n_iter=100):\n",
    "    # This function activates drop-out when doing predictions\n",
    "    f = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                      [model.layers[-1].output])\n",
    "    all_pred = np.zeros((n_iter,) + (stations,look_forward,) )\n",
    "    \n",
    "    for iter in range(n_iter):\n",
    "        all_pred[iter] = np.array( f([dataset.reshape( (1,) + dataset.shape), 1]) ).reshape(66, 24)\n",
    "    avg_pred = all_pred.mean(axis=0)\n",
    "    std = np.std(all_pred, axis=0)\n",
    "    return all_pred, avg_pred, std\n",
    "\n",
    "# given a model and data from the original dataset, it uses predict_with_uncertainty to predict with dropout\n",
    "def run_predictions (model, dataset):\n",
    "    \n",
    "    # scale the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(np.array(dataset))\n",
    "    data = dataset.T\n",
    "    \n",
    "    # feeds the model and data into a function that will activate the dro-out\n",
    "    all_pred, avg_pred, std = predict_with_uncertainty(model, data)\n",
    "    \n",
    "    # All the predictions\n",
    "    for i in range(stations):\n",
    "        all_pred[:,i,:] = scaler.inverse_transform(all_pred[:,i,:])\n",
    "        \n",
    "    # The Average Prediction\n",
    "    avg_pred = scaler.inverse_transform(avg_pred)\n",
    "    # The Standard Deviation At Each T Of Prediction\n",
    "    std = scaler.inverse_transform(std)\n",
    "    \n",
    "    return all_pred, avg_pred, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict (model, day=11, total_days=1, delta=1, show_gradual_error=False, show_graph=True):\n",
    "    # creates matrices to hold the \n",
    "    avg_preds = np.zeros((288*total_days/delta, look_forward))\n",
    "    true_vals = np.zeros((288*total_days/delta, look_forward))\n",
    "    cdf_vals  = np.zeros((288*total_days/delta, look_forward))\n",
    "    \n",
    "    percent_over = np.zeros(288*total_days/delta)\n",
    "    # Runs all the predictions\n",
    "    for i in range (288*total_days/delta):\n",
    "        # the point where the model will predict\n",
    "        point = 288*day + i * delta\n",
    "        \n",
    "        # Gets predictions\n",
    "        all_pred, avg_pred, std = run_predictions (model, dataset_orig[point-look_back:point])\n",
    "        \n",
    "        # Reshapes all of the things\n",
    "        true_val = dataset_orig[point:point+look_forward].reshape(look_forward)\n",
    "        avg_pred = avg_pred.reshape(look_forward)\n",
    "        std = std.reshape(look_forward)\n",
    "        \n",
    "        percent_over[i] = 100.0 * (np.ones((len(all_pred),1) )*true_val - all_pred < 0).sum() / all_pred.size\n",
    "        \n",
    "        # converts the original values to cdf values according to the cumullative distibution functions at t+1, t+2, ..., t+24 (0.0-1.0)\n",
    "        # Adds the values to the overall matrix of cdf\n",
    "        cdf_vals[i,:] = norm.cdf(true_val, loc=avg_pred, scale=std)\n",
    "        \n",
    "        avg_preds[i], true_vals[i] = avg_pred, true_val\n",
    "        \n",
    "        if(show_graph):\n",
    "            # plots the uncertainty to the degree of half a standard deviation\n",
    "            plt.fill_between(range(point, point+look_forward), \n",
    "                             avg_pred + std/2.0, \n",
    "                             avg_pred - std/2.0, \n",
    "                             facecolor='red', alpha=0.25)\n",
    "            # plots the uncertainty to the degree of a full standard deviation\n",
    "            plt.fill_between(range(point, point+look_forward), \n",
    "                             avg_pred + std, \n",
    "                             avg_pred - std, \n",
    "                             facecolor='red', alpha=0.25)\n",
    "            # plots the average predictions\n",
    "            plt.plot(range(point, point+look_forward), avg_pred, color='red', alpha=1)\n",
    "    \n",
    "    print \"Percent Overestimating: {}\".format(np.mean(percent_over))\n",
    "    \n",
    "    if show_graph:\n",
    "        plt.plot( np.arange(288*total_days+look_forward) + 288 * day, dataset_orig[288*day:288*day+288*total_days + look_forward,0])\n",
    "        plt.axhline(0)\n",
    "        plt.show()\n",
    "    \n",
    "    if show_gradual_error:\n",
    "        indiv_err = true_vals - avg_preds\n",
    "        for i in range(look_forward):\n",
    "            plt.scatter(np.ones(len(indiv_err[:,i]))*i,indiv_err[:,i], color='black',alpha=0.1)\n",
    "        plt.axhline(0)\n",
    "        plt.show()\n",
    "    return cdf_vals"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:4 - Expanding Neural Net's Scope.ipynb
   "execution_count": 233,
=======
<<<<<<< HEAD:5 - Expanding Neural Net's Scope.ipynb
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 169,
>>>>>>> f2f9d2af909e4f876ba74a8efc9a15e52a7d0aa1:5 - Expanding Neural Net's Scope.ipynb
   "metadata": {},
>>>>>>> fef3ba7be3d841509f803c94138e8933eb10ecef:4 - Expanding Neural Net's Scope.ipynb
   "outputs": [],
   "source": [
    "# The interval between each dataset (original data in 5 minute intervals)\n",
    "time_grouping = '5min'\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('ignored_assets/paxout_table.csv', engine='python', nrows=288*21)\n",
    "dataframe['time_bucket'] = pd.to_datetime(dataframe['time_bucket'])\n",
    "dataframe = dataframe.set_index('time_bucket')\n",
    "# dataframe['total'] = dataframe.sum(axis=1)\n",
    "dataframe['day_hour'] = dataframe.index.round(time_grouping)\n",
    "dataframe = dataframe.groupby('day_hour').sum()\n",
    "# removes the timestamp at column 67\n",
<<<<<<< HEAD:4 - Expanding Neural Net's Scope.ipynb
    "dataset_orig = dataframe.values[:,:stations]\n",
=======
    "dataset_orig = dataframe.values[:,:]\n",
>>>>>>> f2f9d2af909e4f876ba74a8efc9a15e52a7d0aa1:5 - Expanding Neural Net's Scope.ipynb
    "dataset_orig = dataset_orig.astype('float32')\n",
    "\n",
    "# scale the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6048, 66)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_orig.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:5 - Expanding Neural Net's Scope.ipynb
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2545, 288)\n"
     ]
    }
   ],
=======
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
>>>>>>> fef3ba7be3d841509f803c94138e8933eb10ecef:4 - Expanding Neural Net's Scope.ipynb
   "source": [
    "# split into train and test sets\n",
    "train_size = 288 * 10\n",
    "test_size = len(dataset) - train_size\n",
    "test, train = dataset[0:test_size,:], dataset[test_size:len(dataset)-look_forward,:]\n",
    "\n",
    "# reshape into X=[t, t-1, t-2,..., t-look_back] and Y=[t+1, t+2,... t+look_forward]\n",
    "trainX, trainY = create_dataset(train, look_back, look_forward)\n",
<<<<<<< HEAD:5 - Expanding Neural Net's Scope.ipynb
    "testX, testY = create_dataset(test, look_back, look_forward)\n",
    "print trainX.shape\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
=======
    "testX, testY = create_dataset(test, look_back, look_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a model but does not compile the model\n",
    "def create_model(rate = 0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate, input_shape=(stations, look_back)))\n",
    "    lstm = LSTM(256, recurrent_dropout=0.3, return_sequences=True)\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(look_forward) )\n",
    "    return model"
>>>>>>> fef3ba7be3d841509f803c94138e8933eb10ecef:4 - Expanding Neural Net's Scope.ipynb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:4 - Expanding Neural Net's Scope.ipynb
   "execution_count": 173,
=======
<<<<<<< HEAD:5 - Expanding Neural Net's Scope.ipynb
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0. ,  0. ,  0. , ...,  0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[ 0. ,  0. ,  0. , ...,  0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[ 0. ,  0. ,  0. , ...,  0. ,  0. ,  0.2]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0. ,  0. ,  0. , ...,  0. ,  0. ,  0.2]],\n",
       "\n",
       "       [[ 0. ,  0. ,  0. , ...,  0. ,  0.2,  0.2]],\n",
       "\n",
       "       [[ 0. ,  0. ,  0. , ...,  0.2,  0.2,  0. ]]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
>>>>>>> f2f9d2af909e4f876ba74a8efc9a15e52a7d0aa1:5 - Expanding Neural Net's Scope.ipynb
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "40s - loss: 0.0481\n",
      "Epoch 2/60\n",
      "36s - loss: 0.0250\n",
      "Epoch 3/60\n",
      "37s - loss: 0.0185\n",
      "Epoch 4/60\n",
      "37s - loss: 0.0158\n",
      "Epoch 5/60\n",
      "37s - loss: 0.0142\n",
      "Epoch 6/60\n",
      "37s - loss: 0.0133\n",
      "Epoch 7/60\n",
      "36s - loss: 0.0127\n",
      "Epoch 8/60\n",
      "41s - loss: 0.0123\n",
      "Epoch 9/60\n",
      "43s - loss: 0.0120\n",
      "Epoch 10/60\n",
      "43s - loss: 0.0117\n",
      "Epoch 11/60\n",
      "38s - loss: 0.0115\n",
      "Epoch 12/60\n",
      "38s - loss: 0.0114\n",
      "Epoch 13/60\n",
      "46s - loss: 0.0112\n",
      "Epoch 14/60\n",
      "40s - loss: 0.0111\n",
      "Epoch 15/60\n",
      "36s - loss: 0.0110\n",
      "Epoch 16/60\n",
      "37s - loss: 0.0110\n",
      "Epoch 17/60\n",
      "37s - loss: 0.0109\n",
      "Epoch 18/60\n",
      "37s - loss: 0.0108\n",
      "Epoch 19/60\n",
      "41s - loss: 0.0107\n",
      "Epoch 20/60\n",
      "37s - loss: 0.0107\n",
      "Epoch 21/60\n",
      "38s - loss: 0.0106\n",
      "Epoch 22/60\n",
      "36s - loss: 0.0106\n",
      "Epoch 23/60\n",
      "37s - loss: 0.0105\n",
      "Epoch 24/60\n",
      "38s - loss: 0.0105\n",
      "Epoch 25/60\n",
      "38s - loss: 0.0105\n",
      "Epoch 26/60\n",
      "37s - loss: 0.0104\n",
      "Epoch 27/60\n",
      "36s - loss: 0.0104\n",
      "Epoch 28/60\n",
      "36s - loss: 0.0103\n",
      "Epoch 29/60\n",
      "37s - loss: 0.0104\n",
      "Epoch 30/60\n",
      "37s - loss: 0.0103\n",
      "Epoch 31/60\n",
      "37s - loss: 0.0103\n",
      "Epoch 32/60\n",
      "38s - loss: 0.0103\n",
      "Epoch 33/60\n",
      "37s - loss: 0.0102\n",
      "Epoch 34/60\n",
      "37s - loss: 0.0103\n",
      "Epoch 35/60\n",
      "37s - loss: 0.0102\n",
      "Epoch 36/60\n",
      "38s - loss: 0.0102\n",
      "Epoch 37/60\n",
      "37s - loss: 0.0102\n",
      "Epoch 38/60\n",
      "39s - loss: 0.0101\n",
      "Epoch 39/60\n",
      "36s - loss: 0.0101\n",
      "Epoch 40/60\n",
      "36s - loss: 0.0101\n",
      "Epoch 41/60\n",
      "36s - loss: 0.0101\n",
      "Epoch 42/60\n",
      "37s - loss: 0.0101\n",
      "Epoch 43/60\n",
      "37s - loss: 0.0101\n",
      "Epoch 44/60\n",
      "36s - loss: 0.0101\n",
      "Epoch 45/60\n",
      "41s - loss: 0.0101\n",
      "Epoch 46/60\n",
      "36s - loss: 0.0100\n",
      "Epoch 47/60\n",
      "37s - loss: 0.0100\n",
      "Epoch 48/60\n",
      "37s - loss: 0.0100\n",
      "Epoch 49/60\n",
      "46s - loss: 0.0100\n",
      "Epoch 50/60\n",
      "38s - loss: 0.0100\n",
      "Epoch 51/60\n",
      "36s - loss: 0.0100\n",
      "Epoch 52/60\n",
      "37s - loss: 0.0100\n",
      "Epoch 53/60\n",
      "50s - loss: 0.0100\n",
      "Epoch 54/60\n",
      "46s - loss: 0.0099\n",
      "Epoch 55/60\n",
      "38s - loss: 0.0099\n",
      "Epoch 56/60\n",
      "40s - loss: 0.0099\n",
      "Epoch 57/60\n",
      "39s - loss: 0.0099\n",
      "Epoch 58/60\n",
      "37s - loss: 0.0099\n",
      "Epoch 59/60\n",
      "37s - loss: 0.0099\n",
      "Epoch 60/60\n",
      "38s - loss: 0.0099\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (66, 288) for Tensor u'dropout_61_input:0', which has shape '(?, 66, 288)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-2895803e3ac4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_days\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# cdf_matrix = predict(model, total_days=1,day=11,delta=1, show_graph=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-168-2abe75ac49c8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, day, total_days, delta, show_gradual_error, show_graph)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Gets predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mall_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_predictions\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Reshapes all of the things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-167-bb838475a0f8>\u001b[0m in \u001b[0;36mrun_predictions\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# feeds the model and data into a function that will activate the dro-out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mall_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_uncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# All the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-167-bb838475a0f8>\u001b[0m in \u001b[0;36mpredict_with_uncertainty\u001b[0;34m(model, dataset, n_iter)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mall_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlook_forward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mavg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TomCat/ts-predict/venv/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TomCat/ts-predict/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TomCat/ts-predict/venv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    962\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (66, 288) for Tensor u'dropout_61_input:0', which has shape '(?, 66, 288)'"
     ]
    }
   ],
   "source": [
    "\n",
    "def priorities (y_true, y_pred):\n",
    "    w = np.arange(1,0, -1./24)**1.3\n",
    "    w = w / w.sum()\n",
    "    w = w[:,None]\n",
    "    W = K.variable(value = w)\n",
    "    return K.dot( K.abs(y_pred-y_true), W)\n",
    "\n",
    "batch = 1\n",
    "seed = 11\n",
    "# create and fit the LSTM network\n",
    "model = create_model(rate=0.2)\n",
    "\n",
    "np.random.seed(seed)\n",
    "model.compile(loss=priorities, optimizer=RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "\n",
    "for i in range(1):\n",
    "    model.fit(trainX, trainY, epochs=60, batch_size=60, verbose=2)\n",
    "\n",
    "predict(model, total_days=1,day=11,delta=24)\n",
    "\n",
    "# cdf_matrix = predict(model, total_days=1,day=11,delta=1, show_graph=False)\n",
    "# for i in range (24):\n",
    "#     plt.hist(cdf_matrix[:,i],20)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
<<<<<<< HEAD:4 - Expanding Neural Net's Scope.ipynb
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,24) (66,) (100,24) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-234-2f9da83239f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_days\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-232-2abe75ac49c8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, day, total_days, delta, show_gradual_error, show_graph)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Gets predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mall_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_predictions\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Reshapes all of the things\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-177f2403ce04>\u001b[0m in \u001b[0;36mrun_predictions\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# All the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;31m# The Average Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mavg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/TomCat/ts-predict/venv/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEPRECATION_MSG_1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,24) (66,) (100,24) "
     ]
    }
   ],
   "source": [
    "\n",
    "predict(model, total_days=1,day=11,delta=24)\n"
   ]
=======
   "outputs": [],
>>>>>>> fef3ba7be3d841509f803c94138e8933eb10ecef:4 - Expanding Neural Net's Scope.ipynb
   "source": []
>>>>>>> f2f9d2af909e4f876ba74a8efc9a15e52a7d0aa1:5 - Expanding Neural Net's Scope.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "              [[1,2,3,4,5,6],\n",
    "               [2,2,3,4,5,6],\n",
    "               [3,2,3,4,5,6],\n",
    "               [4,2,3,4,5,6]],\n",
    "              \n",
    "              [[1,2,3,4,5,6],\n",
    "               [2,2,3,4,5,6],\n",
    "               [3,2,3,4,5,6],\n",
    "               [4,2,3,4,5,6]],\n",
    "    \n",
    "              [[1,2,3,4,5,6],\n",
    "               [2,2,3,4,5,6],\n",
    "               [3,2,3,4,5,6],\n",
    "               [4,2,3,4,5,6]],\n",
    "    \n",
    "              [[1,2,3,4,5,6],\n",
    "               [2,2,3,4,5,6],\n",
    "               [3,2,3,4,5,6],\n",
    "               [4,2,3,4,5,6]]])\n",
    "a[:, 2, 2] = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:4 - Expanding Neural Net's Scope.ipynb
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3, 4, 5, 6],\n",
       "        [2, 2, 3, 4, 5, 6],\n",
       "        [3, 2, 1, 4, 5, 6],\n",
       "        [4, 2, 3, 4, 5, 6]],\n",
       "\n",
       "       [[1, 2, 3, 4, 5, 6],\n",
       "        [2, 2, 3, 4, 5, 6],\n",
       "        [3, 2, 2, 4, 5, 6],\n",
       "        [4, 2, 3, 4, 5, 6]],\n",
       "\n",
       "       [[1, 2, 3, 4, 5, 6],\n",
       "        [2, 2, 3, 4, 5, 6],\n",
       "        [3, 2, 3, 4, 5, 6],\n",
       "        [4, 2, 3, 4, 5, 6]],\n",
       "\n",
       "       [[1, 2, 3, 4, 5, 6],\n",
       "        [2, 2, 3, 4, 5, 6],\n",
       "        [3, 2, 4, 4, 5, 6],\n",
       "        [4, 2, 3, 4, 5, 6]]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
<<<<<<< HEAD:5 - Expanding Neural Net's Scope.ipynb
=======
>>>>>>> f2f9d2af909e4f876ba74a8efc9a15e52a7d0aa1:5 - Expanding Neural Net's Scope.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
>>>>>>> fef3ba7be3d841509f803c94138e8933eb10ecef:4 - Expanding Neural Net's Scope.ipynb
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
