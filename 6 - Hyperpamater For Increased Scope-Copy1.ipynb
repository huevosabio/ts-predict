{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import math\n",
    "import keras\n",
    "from scipy.stats  import norm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras        import backend as K\n",
    "from datetime     import datetime\n",
    "from keras.optimizers      import RMSprop\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics       import mean_squared_error\n",
    "from matplotlib.pylab      import rcParams\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "# number of inputs to be fed\n",
    "look_back    = 288\n",
    "# number of outputs to be generated\n",
    "look_forward = 24\n",
    "# the number of stations\n",
    "stations     = 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The interval between each dataset (original data in 5 minute intervals)\n",
    "time_grouping = '5min'\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('ignored_assets/paxout_table.csv', engine='python', nrows=288*21)\n",
    "dataframe['time_bucket'] = pd.to_datetime(dataframe['time_bucket'])\n",
    "dataframe = dataframe.set_index('time_bucket')\n",
    "# dataframe['total'] = dataframe.sum(axis=1)\n",
    "dataframe['day_hour'] = dataframe.index.round(time_grouping)\n",
    "dataframe = dataframe.groupby('day_hour').sum()\n",
    "# removes the timestamp at column 67\n",
    "dataset_orig = dataframe.values[:,:stations]\n",
    "dataset_orig = dataset_orig.astype('float32')\n",
    "# scale the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset_orig)\n",
    "\n",
    "# convert an array of values into a dataset matrix, adjusted to make a dateset that is 66 wide\n",
    "def create_dataset(dataset, look_back=1, look_forward=2):\n",
    "    dataX, dataY = [], []\n",
    "    np.array(dataY)\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),:]\n",
    "        # Makes sure that the array isn't the last 2 or 3 or whatever bits. It has to be the full 24\n",
    "        if len(dataset[i + look_back:i+look_back+look_forward, 0]) == look_forward:\n",
    "            dataX.append(a.T)\n",
    "            dataY.append(dataset[i + look_back:i+look_back+look_forward, :].T)\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = 288*10\n",
    "test_size = len(dataset) - train_size\n",
    "test, train = dataset[0:test_size,:], dataset[test_size:len(dataset)-look_forward,:]\n",
    "\n",
    "# reshape into X=[t, t-1, t-2,..., t-look_back] and Y=[t+1, t+2,... t+look_forward]\n",
    "trainX, trainY = create_dataset(train, look_back, look_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_heavy_loss (y_true, y_pred):\n",
    "    w = np.arange(1,0, -1./24) ** 2\n",
    "    w = (w / w.sum())[:,None]\n",
    "    W = K.variable(value = w)\n",
    "    return K.dot( K.abs(y_pred-y_true), W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# creates a model but does not compile the model\n",
    "def create_model(dropout_rate      = 0.1):\n",
    "    weight_constraint = 1\n",
    "    layers = 2\n",
    "    neurons = 128\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(dropout_rate, input_shape=(stations, look_back)))\n",
    "    lstm = LSTM(neurons, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "    model.add(lstm)\n",
    "    for i in range(layers):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(neurons))\n",
    "    \n",
    "    model.add(Dense(look_forward))\n",
    "    model.compile(loss=top_heavy_loss, optimizer=RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# creates the model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=60, batch_size=40, verbose=3)\n",
    "\n",
    "# The Parameters to Search Through\n",
    "dropout_rate  = [0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "# Defines it as a dictionary\n",
    "param_grid = dict(dropout_rate = dropout_rate)\n",
    "\n",
    "# Begins the Search For the Optimal Properties\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(trainX, trainY)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Given: \n",
    "* epochs = 10\n",
    "* batch_size = 60\n",
    "\n",
    "\n",
    "Best: 0.073481 using {'dropout_rate': 0.5}\n",
    "0.068988 (0.001514) with: {'dropout_rate': 0.1}\n",
    "0.068582 (0.002472) with: {'dropout_rate': 0.2}\n",
    "0.069281 (0.002085) with: {'dropout_rate': 0.3}\n",
    "0.070244 (0.002457) with: {'dropout_rate': 0.4}\n",
    "0.073481 (0.004081) with: {'dropout_rate': 0.5}\n",
    "\n",
    "Best: 0.073881 using {'dropout_rate': 0.5}\n",
    "0.068489 (0.002157) with: {'dropout_rate': 0.1}\n",
    "0.068176 (0.002022) with: {'dropout_rate': 0.2}\n",
    "0.069107 (0.002549) with: {'dropout_rate': 0.3}\n",
    "0.070668 (0.003781) with: {'dropout_rate': 0.4}\n",
    "0.073881 (0.003803) with: {'dropout_rate': 0.5}\n",
    "\n",
    "Best: 0.073639 using {'dropout_rate': 0.5}\n",
    "0.069010 (0.002797) with: {'dropout_rate': 0.2}\n",
    "0.069681 (0.002601) with: {'dropout_rate': 0.3}\n",
    "0.069333 (0.002836) with: {'dropout_rate': 0.35}\n",
    "0.070192 (0.003108) with: {'dropout_rate': 0.4}\n",
    "0.071822 (0.003420) with: {'dropout_rate': 0.45}\n",
    "0.073639 (0.003678) with: {'dropout_rate': 0.5}\n",
    "\n",
    "Best: 0.095888 using {'dropout_rate': 0.7}\n",
    "0.068534 (0.002568) with: {'dropout_rate': 0.3}\n",
    "0.070097 (0.003232) with: {'dropout_rate': 0.4}\n",
    "0.073350 (0.003495) with: {'dropout_rate': 0.5}\n",
    "0.080014 (0.005029) with: {'dropout_rate': 0.6}\n",
    "0.095888 (0.007266) with: {'dropout_rate': 0.7}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "# creates a model but does not compile the model\n",
    "def create_model(layers = 3,\n",
    "                 neurons = 128):\n",
    "    dropout_rate = 0.1\n",
    "    weight_constraint = 1\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(dropout_rate, input_shape=(stations, look_back)))\n",
    "    lstm = LSTM(neurons, recurrent_dropout=dropout_rate, return_sequences=True)\n",
    "    model.add(lstm)\n",
    "    for i in range(layers):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(neurons))\n",
    "    model.add(Dense(look_forward))\n",
    "    model.compile(loss=top_heavy_loss, optimizer=RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# creates the model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=10, batch_size=60, verbose=3)\n",
    "\n",
    "# The Parameters to Search Through\n",
    "layers = [2, 3, 4]\n",
    "neurons  = [128, 256]\n",
    "\n",
    "# Defines it as a dictionary\n",
    "param_grid = dict(layers = layers, neurons = neurons)\n",
    "\n",
    "# Begins the Search For the Optimal Properties\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "\n",
    "grid_result = grid.fit(trainX, trainY)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
