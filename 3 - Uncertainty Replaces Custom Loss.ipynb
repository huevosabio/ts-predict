{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:3em; text-align: center\">Using Uncertainty and Gaussian Distributions to Replace Custom Loss</h1>\n",
    "\n",
    "Instead of using a custom loss function that penalizes over-estimates of the true value, we will use a very accurate neural network and a Gaussian distribution to choose the lower percentiles of the predictions rather than the average prediction (which will try to get as close to the true value as possible). This way, we can select what number in the uncertainty will be used in the real world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas            as pd\n",
    "import math\n",
    "import keras\n",
    "from scipy.stats  import norm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras        import backend as K\n",
    "from datetime     import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics       import mean_squared_error\n",
    "from matplotlib.pylab      import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "\n",
    "# number of inputs to be fed\n",
    "look_back = 288\n",
    "# number of outputs to be generated\n",
    "look_forward = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_uncertainty(model, dataset, n_iter=100):\n",
    "    # This function activates drop-out when doing predictions\n",
    "    f = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                      [model.layers[-1].output])\n",
    "    all_pred = np.zeros((n_iter,) + (1,look_forward,))\n",
    "    for iter in range(n_iter):\n",
    "        all_pred[iter] = f([dataset, 1])\n",
    "    avg_pred = all_pred.mean(axis=0)\n",
    "    std = np.std(all_pred, axis=0)\n",
    "    return all_pred, avg_pred, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a model and data from the original dataset, it uses predict_with_uncertainty to \n",
    "def run_predictions (model, dataset):\n",
    "    # normalizes the dataset and prepares it to be fed to the model\n",
    "    dataset = np.array(dataset).transpose().reshape(-1, 1)\n",
    "    dataset = scaler.fit_transform(dataset).transpose()\n",
    "    data = (dataset.T)\n",
    "    data = data.reshape(1, 1, data.shape[0])\n",
    "    # feeds the model and data into a function that will activate the dro-out\n",
    "    all_pred, avg_pred, std = predict_with_uncertainty(model, data)\n",
    "    \n",
    "    # All the predictions\n",
    "    all_pred = scaler.inverse_transform(all_pred.reshape(all_pred.shape[0], all_pred.shape[-1]))\n",
    "    # The Average Prediction\n",
    "    avg_pred = scaler.inverse_transform(avg_pred)\n",
    "    # The Standard Deviation At Each T Of Prediction\n",
    "    std = scaler.inverse_transform(std)\n",
    "    \n",
    "    return all_pred, avg_pred, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict (model=model, day=11, total_days=1, delta=1, show_gradual_error=True):\n",
    "    # creates matrices to hold the \n",
    "    avg_preds = np.zeros((288*total_days/delta, look_forward))\n",
    "    true_vals = np.zeros((288*total_days/delta, look_forward))\n",
    "    \n",
    "    percent_over = np.zeros(288*total_days/delta)\n",
    "    # Runs all the predictions\n",
    "    for i in range (288*total_days/delta):\n",
    "        # the point where the model will predict\n",
    "        point = 288*day + i * delta\n",
    "        \n",
    "        # Gets predictions\n",
    "        all_pred, avg_pred, std = run_predictions (model, dataset_orig[point-look_back:point].reshape(look_back))\n",
    "        \n",
    "        # Reshapes all of the things\n",
    "        true_val = dataset_orig[point:point+look_forward].reshape(look_forward)\n",
    "        avg_pred = avg_pred.reshape(look_forward)\n",
    "        std = std.reshape(look_forward)\n",
    "        \n",
    "        percent_over[i] = 100.0 * (np.ones((len(all_pred),1) )*true_val - all_pred < 0).sum() / all_pred.size\n",
    "        # plots the uncertainty to the degree of half a standard deviation\n",
    "        plt.fill_between(range(point, point+look_forward), \n",
    "                         avg_pred + std/2.0, \n",
    "                         avg_pred - std/2.0, \n",
    "                         facecolor='red', alpha=0.25)\n",
    "        \n",
    "        # plots the uncertainty to the degree of a full standard deviation\n",
    "        plt.fill_between(range(point, point+look_forward), \n",
    "                         avg_pred + std, \n",
    "                         avg_pred - std, \n",
    "                         facecolor='red', alpha=0.25)\n",
    "        \n",
    "        # plots the predictions\n",
    "        plt.plot(range(point, point+look_forward), avg_pred, color='red', alpha=1)\n",
    "        \n",
    "        avg_preds[i], true_vals[i] = avg_pred, true_val\n",
    "        \n",
    "    # Reshapes the predictions and true vals to calculate the error\n",
    "    plt.plot( np.arange(288*total_days+look_forward) + 288 * day, dataset_orig[288*day:288*day+288*total_days + look_forward,0])\n",
    "    plt.axhline(0)\n",
    "    plt.show()\n",
    "    if show_gradual_error:\n",
    "        indiv_err = true_vals - avg_preds\n",
    "        for i in range(look_forward):\n",
    "            plt.scatter(np.ones(len(indiv_err[:,i]))*i,indiv_err[:,i], color='black',alpha=0.1)\n",
    "        plt.axhline(0)\n",
    "        plt.show()\n",
    "        print \"Percent Overestimating: {}\".format(np.mean(percent_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The interval between each dataset (original data in 5 minute intervals)\n",
    "time_grouping = '5min'\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv('ignored_assets/paxout_table.csv', engine='python', nrows=288*21)\n",
    "dataframe['time_bucket'] = pd.to_datetime(dataframe['time_bucket'])\n",
    "dataframe = dataframe.set_index('time_bucket')\n",
    "dataframe['total'] = dataframe.sum(axis=1)\n",
    "dataframe['day_hour'] = dataframe.index.round(time_grouping)\n",
    "\n",
    "# The data set with the sum of all the cars out at the \n",
    "dataframe = dataframe[['total','day_hour']].groupby('day_hour').sum()\n",
    "\n",
    "dataset_orig = dataframe.values\n",
    "dataset_orig = dataset_orig.astype('float32')\n",
    "\n",
    "# scale the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset_orig)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = 288*14\n",
    "test_size = len(dataset) - train_size\n",
    "test, train = dataset[0:test_size,:], dataset[test_size:len(dataset)-look_forward,:]\n",
    "\n",
    "# reshape into X=[t, t-1, t-2,..., t-look_back] and Y=[t+1, t+2,... t+look_forward]\n",
    "trainX, trainY = create_dataset(train, look_back, look_forward)\n",
    "testX, testY = create_dataset(test, look_back, look_forward)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates a model but does not compile the model\n",
    "def create_model(rate = 0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate, input_shape=(1, look_back)))\n",
    "    lstm = LSTM(256, recurrent_dropout=0.3)\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dense(look_forward))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch = 1\n",
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "# create and fit the LSTM network\n",
    "model = create_model(rate=0.2)\n",
    "model.compile(loss=over_penalty, optimizer=keras.optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=1e-08, decay=0.0002))\n",
    "model.fit(trainX, trainY, epochs=5, batch_size=batch, verbose=2)\n",
    "for i in range (20):\n",
    "    model.fit(trainX, trainY, epochs=1, batch_size=batch, verbose=2)\n",
    "\n",
    "predict(model, total_days=1,day=11,delta=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
